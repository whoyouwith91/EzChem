{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dongdongzhang/Desktop/group/databases/EzChem/ez_chem\n"
     ]
    }
   ],
   "source": [
    "cd ../../ez_chem/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create molecular graphs for delaney datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from featurization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in original delaney file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "delaney = pd.read_csv('../datasets/deepchem/delaney.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>logSolubility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OCC3OC(OCC2OC(OC(C#N)c1ccccc1)C(O)C(O)C2O)C(O)...</td>\n",
       "      <td>-0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cc1occc1C(=O)Nc2ccccc2</td>\n",
       "      <td>-3.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC(C)=CCCC(C)=CC(=O)</td>\n",
       "      <td>-2.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c1ccc2c(c1)ccc3c2ccc4c5ccccc5ccc43</td>\n",
       "      <td>-7.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c1ccsc1</td>\n",
       "      <td>-1.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  logSolubility\n",
       "0  OCC3OC(OCC2OC(OC(C#N)c1ccccc1)C(O)C(O)C2O)C(O)...          -0.77\n",
       "1                             Cc1occc1C(=O)Nc2ccccc2          -3.30\n",
       "2                               CC(C)=CCCC(C)=CC(=O)          -2.06\n",
       "3                 c1ccc2c(c1)ccc3c2ccc4c5ccccc5ccc43          -7.87\n",
       "4                                            c1ccsc1          -1.33"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delaney.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1128, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delaney.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a function to process the compounds. This function is copied from the script file 'makeGraph.py'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genGraphs(all_data, smilesColumn, targetColumn):\n",
    "    examples = []\n",
    "    for idx, smi, tar in zip(range(all_data.shape[0]), all_data[smilesColumn], all_data[targetColumn]):\n",
    "        molgraphs = {}\n",
    "\n",
    "        mol_graph = MolGraph(smi, '1-GNN')\n",
    "        molgraphs['x'] = torch.FloatTensor(mol_graph.f_atoms) # atom features \n",
    "        molgraphs['edge_attr'] = torch.FloatTensor(mol_graph.real_f_bonds) # bond features \n",
    "        molgraphs['edge_index'] = torch.LongTensor(np.concatenate([mol_graph.at_begin, mol_graph.at_end]).reshape(2,-1))\n",
    "        molgraphs['smiles'] = smi\n",
    "        molgraphs['id'] = torch.FloatTensor([idx])\n",
    "        molgraphs['y'] = torch.FloatTensor([tar])\n",
    "        examples.append(molgraphs)\n",
    "        if idx % 100 == 0:\n",
    "            print('Finish processing {} compounds'.format(idx))\n",
    "    print('Done.')\n",
    "        \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish processing 0 compounds\n",
      "Finish processing 100 compounds\n",
      "Finish processing 200 compounds\n",
      "Finish processing 300 compounds\n",
      "Finish processing 400 compounds\n",
      "Finish processing 500 compounds\n",
      "Finish processing 600 compounds\n",
      "Finish processing 700 compounds\n",
      "Finish processing 800 compounds\n",
      "Finish processing 900 compounds\n",
      "Finish processing 1000 compounds\n",
      "Finish processing 1100 compounds\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "graphs = genGraphs(delaney, 'smiles', 'logSolubility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(graphs, '../examples/propertyPrediction/delaney/raw/temp.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data loader for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have prepared the file storing the molecular graph for the datasets, we are good to create the loaders for the next steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our framework, we need to define the sizes for train and valid because it's convinient for CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'dataset': 'deepchem/delaney', # dataset name\n",
    "          'model': '1-GNN',  # model \n",
    "          'train_type': 'from_scratch', \n",
    "          'normalize': False,\n",
    "          'train_size': 902, \n",
    "          'val_size': 113, \n",
    "          'batch_size': 32,\n",
    "          'data_path': '../examples/propertyPrediction/delaney/'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_data_loader(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can check if the create the loader for train/valid sets correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "902\n",
      "113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(loader.train_loader.dataset)), print(len(loader.val_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare models for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we still need to claim a dictionary to control the model calling and training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_train = {'num_layer': 3, # atom embedding layers \n",
    "                'emb_dim':64, # embedding dimension\n",
    "                'NumOutLayers': 3, # number of read-out layers\n",
    "                'num_tasks':1, \n",
    "                'pooling': 'sum',\n",
    "                'gnn_type': 'gcn', \n",
    "                'optimizer': 'adam',\n",
    "                'lr': 0.001,\n",
    "                'loss': 'l2',\n",
    "                'metrics': 'l2', \n",
    "                'weights': 'xavier_norm', # weights initialization method \n",
    "                'taskType': 'single',\n",
    "                'device': torch.device('cpu')\n",
    "                \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(config_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'deepchem/delaney',\n",
       " 'model': '1-GNN',\n",
       " 'train_type': 'from_scratch',\n",
       " 'normalize': False,\n",
       " 'train_size': 902,\n",
       " 'val_size': 113,\n",
       " 'batch_size': 32,\n",
       " 'data_path': '../examples/propertyPrediction/delaney/',\n",
       " 'num_layer': 3,\n",
       " 'emb_dim': 64,\n",
       " 'NumOutLayers': 3,\n",
       " 'num_tasks': 1,\n",
       " 'pooling': 'sum',\n",
       " 'gnn_type': 'gcn',\n",
       " 'optimizer': 'adam',\n",
       " 'lr': 0.001,\n",
       " 'loss': 'l2',\n",
       " 'metrics': 'l2',\n",
       " 'weights': 'xavier_norm',\n",
       " 'device': device(type='cpu'),\n",
       " 'taskType': 'single'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the corresponding model: base model is GCN and entire model acchitecutre is 1-GNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "genModel = get_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "models.GNN_1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = objectview(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genModel(args.num_layer, args.emb_dim, args.NumOutLayers, args.num_tasks, graph_pooling=args.pooling, gnn_type=args.gnn_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNN_1(\n",
       "  (gnn): GNN(\n",
       "    (x_embedding1): Sequential(\n",
       "      (0): Linear(in_features=40, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (gnns): ModuleList(\n",
       "      (0): GCNConv(\n",
       "        (edge_embedding1): Embedding(6, 64)\n",
       "        (edge_embedding2): Embedding(3, 64)\n",
       "      )\n",
       "      (1): GCNConv(\n",
       "        (edge_embedding1): Embedding(6, 64)\n",
       "        (edge_embedding2): Embedding(3, 64)\n",
       "      )\n",
       "      (2): GCNConv(\n",
       "        (edge_embedding1): Embedding(6, 64)\n",
       "        (edge_embedding2): Embedding(3, 64)\n",
       "      )\n",
       "    )\n",
       "    (batch_norms): ModuleList(\n",
       "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (outLayers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (4): Linear(in_features=8, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_weights(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 1.515937143804272 and test loss is 1.5047292860018413 at epoch 1\n",
      "Train loss is 1.2208003867502462 and test loss is 1.6386668554145036 at epoch 2\n",
      "Train loss is 1.0610275162949747 and test loss is 1.4435607191580964 at epoch 3\n",
      "Train loss is 1.165612436998998 and test loss is 1.5613314264227747 at epoch 4\n",
      "Train loss is 0.8835743680222897 and test loss is 1.2666348796336075 at epoch 5\n",
      "Train loss is 0.831746621317948 and test loss is 1.1807890037896873 at epoch 6\n",
      "Train loss is 0.7610444825965573 and test loss is 1.1733822602579473 at epoch 7\n",
      "Train loss is 0.735870380945031 and test loss is 1.1663826982863277 at epoch 8\n",
      "Train loss is 0.9094318241049489 and test loss is 1.3051373283067749 at epoch 9\n",
      "Train loss is 0.7911411515466328 and test loss is 1.2335417144024596 at epoch 10\n",
      "Train loss is 0.7085760532848445 and test loss is 1.1155068620806687 at epoch 11\n",
      "Train loss is 0.6889282043793368 and test loss is 0.9550970032743296 at epoch 12\n",
      "Train loss is 0.7626299762948007 and test loss is 1.2164299719426186 at epoch 13\n",
      "Train loss is 0.7620174506985173 and test loss is 1.1695927585285004 at epoch 14\n",
      "Train loss is 0.6853321097682478 and test loss is 1.0930637386510658 at epoch 15\n",
      "Train loss is 0.651309947368414 and test loss is 0.9769840093746786 at epoch 16\n",
      "Train loss is 0.620600738627418 and test loss is 0.9620448609319088 at epoch 17\n",
      "Train loss is 0.5920844056264933 and test loss is 1.0125015941654418 at epoch 18\n",
      "Train loss is 0.6251379099326906 and test loss is 1.0410361225336033 at epoch 19\n",
      "Train loss is 0.6566141571392924 and test loss is 1.0631840130968162 at epoch 20\n",
      "Train loss is 0.6049108087985016 and test loss is 0.923217152029512 at epoch 21\n",
      "Train loss is 0.6066529537404755 and test loss is 1.0401180326899826 at epoch 22\n",
      "Train loss is 0.6264438164765626 and test loss is 1.0750235577153389 at epoch 23\n",
      "Train loss is 0.5738258055347284 and test loss is 0.9557327492185104 at epoch 24\n",
      "Train loss is 0.5768847177664427 and test loss is 1.0158485166642215 at epoch 25\n",
      "Train loss is 0.628293657196951 and test loss is 0.9714663477422596 at epoch 26\n",
      "Train loss is 0.5707540501113736 and test loss is 1.0103700109243245 at epoch 27\n",
      "Train loss is 0.5675173680340758 and test loss is 0.9474673026993046 at epoch 28\n",
      "Train loss is 0.5525495941463696 and test loss is 0.9698235808030357 at epoch 29\n",
      "Train loss is 0.5760150243336164 and test loss is 0.9404696478825935 at epoch 30\n",
      "Train loss is 0.5859807062113792 and test loss is 1.0060657474692676 at epoch 31\n",
      "Train loss is 0.6129529037586042 and test loss is 0.8979382330184603 at epoch 32\n",
      "Train loss is 0.5131877750186464 and test loss is 0.9401640551539316 at epoch 33\n",
      "Train loss is 0.4930755838018678 and test loss is 0.9050480665962327 at epoch 34\n",
      "Train loss is 0.5558783682756946 and test loss is 0.902178530408974 at epoch 35\n",
      "Train loss is 0.5691327997113462 and test loss is 0.9706249243938272 at epoch 36\n",
      "Train loss is 0.5236521038982401 and test loss is 0.8435638010354575 at epoch 37\n",
      "Train loss is 0.5305012025864442 and test loss is 0.8636692607810744 at epoch 38\n",
      "Train loss is 0.48753800855311324 and test loss is 0.8440171984280715 at epoch 39\n",
      "Train loss is 0.4999341921414937 and test loss is 0.9063798548303124 at epoch 40\n",
      "Train loss is 0.5892758193155485 and test loss is 1.0659756751872635 at epoch 41\n",
      "Train loss is 0.48362712652488726 and test loss is 0.8718467160904645 at epoch 42\n",
      "Train loss is 0.47385358875658984 and test loss is 0.8942124910750471 at epoch 43\n",
      "Train loss is 0.46082655331152156 and test loss is 0.8876864492647457 at epoch 44\n",
      "Train loss is 0.49644369263324306 and test loss is 0.8932266717251262 at epoch 45\n",
      "Train loss is 0.525301872613395 and test loss is 0.9611742921935412 at epoch 46\n",
      "Train loss is 0.6336803607562969 and test loss is 0.9699835768027043 at epoch 47\n",
      "Train loss is 0.6458476408532386 and test loss is 0.9744575397504915 at epoch 48\n",
      "Train loss is 0.4879277742779115 and test loss is 0.8637297142947983 at epoch 49\n",
      "Train loss is 0.5454400173062288 and test loss is 0.9835908279193744 at epoch 50\n",
      "Train loss is 0.5839495696345737 and test loss is 0.9562414530453157 at epoch 51\n",
      "Train loss is 0.4481104904549853 and test loss is 0.8496701975130887 at epoch 52\n",
      "Train loss is 0.49493081160789204 and test loss is 0.8568675480357869 at epoch 53\n",
      "Train loss is 0.5370611828077907 and test loss is 0.8633121122479354 at epoch 54\n",
      "Train loss is 0.5402920839016789 and test loss is 0.9480132623159266 at epoch 55\n",
      "Train loss is 0.458983547128783 and test loss is 0.8418928122671596 at epoch 56\n",
      "Train loss is 0.48406995889471593 and test loss is 0.8161383376338234 at epoch 57\n",
      "Train loss is 0.47752575377971035 and test loss is 0.8711613855490746 at epoch 58\n",
      "Train loss is 0.5228444340832327 and test loss is 0.8869342290067862 at epoch 59\n",
      "Train loss is 0.4956765260165853 and test loss is 0.930545539153614 at epoch 60\n",
      "Train loss is 0.4568865620393407 and test loss is 0.8468617878126073 at epoch 61\n",
      "Train loss is 0.41816733446760945 and test loss is 0.8228276160300843 at epoch 62\n",
      "Train loss is 0.42537615692783964 and test loss is 0.8652982666674532 at epoch 63\n",
      "Train loss is 0.40467717072843856 and test loss is 0.8717620065407609 at epoch 64\n",
      "Train loss is 0.4278938135652002 and test loss is 0.8653756880911053 at epoch 65\n",
      "Train loss is 0.4676874037847913 and test loss is 0.9497435574997111 at epoch 66\n",
      "Train loss is 0.4793818360429373 and test loss is 0.9026278424262489 at epoch 67\n",
      "Train loss is 0.4479714533841032 and test loss is 0.8961522104072692 at epoch 68\n",
      "Train loss is 0.42944939692049966 and test loss is 0.8429627454542068 at epoch 69\n",
      "Train loss is 0.44910472583743316 and test loss is 0.854014305544352 at epoch 70\n",
      "Train loss is 0.44246022808091834 and test loss is 0.9221404388529415 at epoch 71\n",
      "Train loss is 0.4312814970538873 and test loss is 0.9018707356712315 at epoch 72\n",
      "Train loss is 0.416860304978088 and test loss is 0.8356007762356591 at epoch 73\n",
      "Train loss is 0.4226347122768814 and test loss is 0.8766505477665973 at epoch 74\n",
      "Train loss is 0.4328758035080073 and test loss is 0.7565576841696645 at epoch 75\n",
      "Train loss is 0.4606196067007775 and test loss is 0.808695210896002 at epoch 76\n",
      "Train loss is 0.4319396040772326 and test loss is 0.8699188177554362 at epoch 77\n",
      "Train loss is 0.43611978049310607 and test loss is 0.8434267837844314 at epoch 78\n",
      "Train loss is 0.5141649671657345 and test loss is 0.9670016909883343 at epoch 79\n",
      "Train loss is 0.4171166870653763 and test loss is 0.8978475209199742 at epoch 80\n",
      "Train loss is 0.4527268140028175 and test loss is 0.890740470762455 at epoch 81\n",
      "Train loss is 0.5882373180424709 and test loss is 1.0763229568688526 at epoch 82\n",
      "Train loss is 0.387960789875743 and test loss is 0.854793857184577 at epoch 83\n",
      "Train loss is 0.41641180668141825 and test loss is 0.821146114577886 at epoch 84\n",
      "Train loss is 0.4236636391974737 and test loss is 0.8540849336426836 at epoch 85\n",
      "Train loss is 0.4235931483440003 and test loss is 0.8420878038157152 at epoch 86\n",
      "Train loss is 0.5170722648651181 and test loss is 1.0214355923909624 at epoch 87\n",
      "Train loss is 0.41699100824765556 and test loss is 0.8474531150047152 at epoch 88\n",
      "Train loss is 0.4590180891925069 and test loss is 0.9083565203014126 at epoch 89\n",
      "Train loss is 0.4154619822214532 and test loss is 0.8946416195408659 at epoch 90\n",
      "Train loss is 0.3817897095465456 and test loss is 0.841000633974324 at epoch 91\n",
      "Train loss is 0.40649170287721553 and test loss is 0.8098847039900303 at epoch 92\n",
      "Train loss is 0.41027121976129344 and test loss is 0.8691475514339526 at epoch 93\n",
      "Train loss is 0.41150440086940837 and test loss is 0.8998463406784671 at epoch 94\n",
      "Train loss is 0.42823507884676854 and test loss is 0.7885976498550662 at epoch 95\n",
      "Train loss is 0.3819435338770483 and test loss is 0.866021549546999 at epoch 96\n",
      "Train loss is 0.3778873193141602 and test loss is 0.7861590078977025 at epoch 97\n",
      "Train loss is 0.4000323669928175 and test loss is 0.8293568068853937 at epoch 98\n",
      "Train loss is 0.5072189970924839 and test loss is 0.9747455325986755 at epoch 99\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 100):\n",
    "    _ = train(model, optimizer, loader.train_loader, config)\n",
    "    train_error = test(model, loader.train_loader, config)\n",
    "    test_error = test(model, loader.val_loader, config)\n",
    "    print(\"Train loss is {} and test loss is {} at epoch {}\".format(train_error, test_error, epoch))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
