{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepPurpose Deep Dive\n",
    "## Tutorial 1: Training a Drug-Target Interaction Model from Scratch\n",
    "#### [@KexinHuang5](https://twitter.com/KexinHuang5)\n",
    "\n",
    "In this tutorial, we take a deep dive into DeepPurpose and show how it builds a drug-target interaction model from scratch. \n",
    "\n",
    "Agenda:\n",
    "\n",
    "- Part I: Overview of DeepPurpose and Data\n",
    "- Part II: Drug Target Interaction Prediction\n",
    "    - DeepPurpose Framework\n",
    "    - Applications to Drug Repurposing and Virtual Screening\n",
    "    - Pretrained Models\n",
    "    - Hyperparameter Tuning\n",
    "    - Model Robustness Evaluation\n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepPurpose import utils, dataset\n",
    "from DeepPurpose import DTI as models\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Overview of DeepPurpose and Data\n",
    "\n",
    "Drug-target interaction measures the binding of drug molecules to the protein targets. Accurate identification of DTI is fundamental for drug discovery and supports many downstream tasks. Among others, drug screening and repurposing are two main applications based on DTI. Drug screening helps identify ligand candidates that can bind to the protein of interest, whereas drug repurposing finds new therapeutic purposes for existing drugs. Both tasks could alleviate the costly, time-consuming, and labor-intensive process of synthesis and analysis, which is extremely important, especially in the cases of hunting effective and safe treatments for COVID-19.\n",
    "\n",
    "DeepPurpose is a pytorch-based deep learning framework that is initiated to provide a simple but powerful toolkit for drug-target interaction prediction and its related applications. We see many exciting recent works in this direction, but to leverage these models, it takes lots of efforts due to the esoteric instructions and interface. DeepPurpose is designed to make things as simple as possible using a unified framework.\n",
    "\n",
    "DeepPurpose uses an encoder-decoder framework. Drug repurposing and screening are two applications after we obtain DTI models. The input to the model is a drug target pair, where drug uses the simplified molecular-input line-entry system (SMILES) string and target uses the amino acid sequence. The output is a score indicating the binding activity of the drug target pair. Now, we begin talking about the data format expected.\n",
    "\n",
    "\n",
    "(**Data**) DeepPurpose takes into an array of drug's SMILES strings (**d**), an array of target protein's amino acid sequence (**t**), and an array of label (**y**), which can either be binary 0/1 indicating interaction outcome or a real number indicating affinity value. The input drug and target arrays should be paired, i.e. **y**\\[0\\] is the score for **d**\\[0\\] and **t**\\[0\\].\n",
    "\n",
    "Besides transforming into numpy arrays through some data wrangling on your own, DeepPurpose also provides two ways to help data preparation. \n",
    "\n",
    "The first way is to read from local files. For example, to load drug target pairs, we expect a file.txt where each line is a drug SMILES string, followed by a protein sequence, and an affinity score or 0/1 label:\n",
    "\n",
    "```CC1=C...C4)N MKK...LIDL 7.365``` \\\n",
    "```CC1=C...C4)N QQP...EGKH 4.999```\n",
    "\n",
    "Then, we use ```dataset.read_file_training_dataset_drug_target_pairs``` to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug 1: CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC=C4)N\n",
      "Target 1: MKKFFDSRREQGGSGLGSGSSGGGGSTSGLGSGYIGRVFGIGRQQVTVDEVLAEGGFAIVFLVRTSNGMKCALKRMFVNNEHDLQVCKREIQIMRDLSGHKNIVGYIDSSINNVSSGDVWEVLILMDFCRGGQVVNLMNQRLQTGFTENEVLQIFCDTCEAVARLHQCKTPIIHRDLKVENILLHDRGHYVLCDFGSATNKFQNPQTEGVNAVEDEIKKYTTLSYRAPEMVNLYSGKIITTKADIWALGCLLYKLCYFTLPFGESQVAICDGNFTIPDNSRYSQDMHCLIRYMLEPDPDKRPDIYQVSYFSFKLLKKECPIPNVQNSPIPAKLPEPVKASEAAAKKTQPKARLTDPIPTTETSIAPRQRPKAGQTQPNPGILPIQPALTPRKRATVQPPPQAAGSSNQPGLLASVPQPKPQAPPSQPLPQTQAKQPQAPPTPQQTPSTQAQGLPAQAQATPQHQQQLFLKQQQQQQQPPPAQQQPAGTFYQQQQAQTQQFQAVHPATQKPAIAQFPVVSQGGSQQQLMQNFYQQQQQQQQQQQQQQLATALHQQQLMTQQAALQQKPTMAAGQQPQPQPAAAPQPAPAQEPAIQAPVRQQPKVQTTPPPAVQGQKVGSLTPPSSPKTQRAGHRRILSDVTHSAVFGVPASKSTQLLQAAAAEASLNKSKSATTTPSGSPRTSQQNVYNPSEGSTWNPFDDDNFSKLTAEELLNKDFAKLGEGKHPEKLGGSAESLIPGFQSTQGDAFATTSFSAGTAEKRKGGQTVDSGLPLLSVSDPFIPLQVPDAPEKLIEGLKSPDTSLLLPDLLPMTDPFGSTSDAVIEKADVAVESLIPGLEPPVPQRLPSQTESVTSNRTDSLTGEDSLLDCSLLSNPTTDLLEEFAPTAISAPVHKAAEDSNLISGFDVPEGSDKVAEDEFDPIPVLITKNPQGGHSRNSSGSSESSLPNLARSLLLVDQLIDL\n",
      "Score 1: 7.365\n"
     ]
    }
   ],
   "source": [
    "X_drugs, X_targets, y = dataset.read_file_training_dataset_drug_target_pairs('./toy_data/dti.txt')\n",
    "print('Drug 1: ' + X_drugs[0])\n",
    "print('Target 1: ' + X_targets[0])\n",
    "print('Score 1: ' + str(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many method researchers want to test on benchmark datasets such as KIBA/DAVIS/BindingDB, DeepPurpose also provides data loaders to ease preprocessing. For example, we want to load the DAVIS dataset, we can use ```dataset.load_process_DAVIS```. It will download, preprocess to the designated data format. It supports label log-scale transformation for easier regression and also allows label binarization given a customized threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Processing...\n",
      "100% [............................................................................] 179878 / 179878Beginning to extract zip file...\n",
      "Default set to logspace (nM -> p) for easier regression\n",
      "Done!\n",
      "Drug 1: CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC=C4)N\n",
      "Target 1: MKKFFDSRREQGGSGLGSGSSGGGGSTSGLGSGYIGRVFGIGRQQVTVDEVLAEGGFAIVFLVRTSNGMKCALKRMFVNNEHDLQVCKREIQIMRDLSGHKNIVGYIDSSINNVSSGDVWEVLILMDFCRGGQVVNLMNQRLQTGFTENEVLQIFCDTCEAVARLHQCKTPIIHRDLKVENILLHDRGHYVLCDFGSATNKFQNPQTEGVNAVEDEIKKYTTLSYRAPEMVNLYSGKIITTKADIWALGCLLYKLCYFTLPFGESQVAICDGNFTIPDNSRYSQDMHCLIRYMLEPDPDKRPDIYQVSYFSFKLLKKECPIPNVQNSPIPAKLPEPVKASEAAAKKTQPKARLTDPIPTTETSIAPRQRPKAGQTQPNPGILPIQPALTPRKRATVQPPPQAAGSSNQPGLLASVPQPKPQAPPSQPLPQTQAKQPQAPPTPQQTPSTQAQGLPAQAQATPQHQQQLFLKQQQQQQQPPPAQQQPAGTFYQQQQAQTQQFQAVHPATQKPAIAQFPVVSQGGSQQQLMQNFYQQQQQQQQQQQQQQLATALHQQQLMTQQAALQQKPTMAAGQQPQPQPAAAPQPAPAQEPAIQAPVRQQPKVQTTPPPAVQGQKVGSLTPPSSPKTQRAGHRRILSDVTHSAVFGVPASKSTQLLQAAAAEASLNKSKSATTTPSGSPRTSQQNVYNPSEGSTWNPFDDDNFSKLTAEELLNKDFAKLGEGKHPEKLGGSAESLIPGFQSTQGDAFATTSFSAGTAEKRKGGQTVDSGLPLLSVSDPFIPLQVPDAPEKLIEGLKSPDTSLLLPDLLPMTDPFGSTSDAVIEKADVAVESLIPGLEPPVPQRLPSQTESVTSNRTDSLTGEDSLLDCSLLSNPTTDLLEEFAPTAISAPVHKAAEDSNLISGFDVPEGSDKVAEDEFDPIPVLITKNPQGGHSRNSSGSSESSLPNLARSLLLVDQLIDL\n",
      "Score 1: 7.366531544420414\n"
     ]
    }
   ],
   "source": [
    "X_drugs, X_targets, y = dataset.load_process_DAVIS(path = './data', binary = False, convert_to_log = True, threshold = 30)\n",
    "print('Drug 1: ' + X_drugs[0])\n",
    "print('Target 1: ' + X_targets[0])\n",
    "print('Score 1: ' + str(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more detailed examples and tutorials of data loading, checkout this [tutorial](./DEMO/load_data_tutorial.ipynb).\n",
    "\n",
    "## Part II: Drug Target Interaction Prediction Framework\n",
    "\n",
    "DeepPurpose provides a simple framework to conduct DTI research using 8 encoders for drugs and 7 for proteins. It basically consists of the following steps, where each step corresponds to one line of code:\n",
    "\n",
    "- Encoder specification\n",
    "- Data encoding and split\n",
    "- Model configuration generation\n",
    "- Model initialization\n",
    "- Model Training\n",
    "- Model Prediction and Repuposing/Screening\n",
    "- Model Saving and Loading\n",
    "\n",
    "Let's start with data encoding! \n",
    "\n",
    "(**Encoder specification**) After we obtain the required data format from Part I, we need to prepare them for the encoders. Hence, we first specify the encoder to use for drug and protein. Here we try MPNN for drug and CNN for target.\n",
    "\n",
    "If you find MPNN and CNN are too large for the CPUs, you can try smaller encoders by uncommenting the last line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_encoding, target_encoding = 'Transformer', 'Transformer'\n",
    "#drug_encoding, target_encoding = 'Morgan', 'Conjoint_triad'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can switch encoder just by changing the encoding name above. The full list of encoders are listed [here](https://github.com/kexinhuang12345/DeepPurpose#encodings). Here, we are using the message passing neural network encoder for drug and convolutional neural network encoder for protein.\n",
    "\n",
    "(**Data encoding and split**) Now, we encode the data into the specified format, using ```utils.data_process``` function. It specifies train/validation/test split fractions, and random seed to ensure same data splits for reproducibility. This function also support data splitting methods such as ```cold_drug``` and ```cold_protein```, which splits on drug/proteins for model robustness evaluation to test on unseen drug/proteins.\n",
    "\n",
    "The function outputs train, val, test pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug Target Interaction Prediction Mode...\n",
      "in total: 30056 drug-target pairs\n",
      "encoding drug...\n",
      "unique drugs: 68\n",
      "encoding protein...\n",
      "unique target sequence: 379\n",
      "splitting dataset...\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Target Sequence</th>\n",
       "      <th>Label</th>\n",
       "      <th>drug_encoding</th>\n",
       "      <th>target_encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
       "      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>([800, 122, 248, 282, 623, 272, 1256, 2210, 91...</td>\n",
       "      <td>([266, 23, 513, 143, 306, 78, 182, 250, 2325, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
       "      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>([800, 122, 248, 282, 623, 272, 1256, 2210, 91...</td>\n",
       "      <td>([266, 23, 513, 143, 306, 78, 182, 250, 2325, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
       "      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>([800, 122, 248, 282, 623, 272, 1256, 2210, 91...</td>\n",
       "      <td>([266, 23, 513, 143, 306, 78, 182, 250, 2325, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
       "      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>([800, 122, 248, 282, 623, 272, 1256, 2210, 91...</td>\n",
       "      <td>([266, 23, 513, 143, 306, 78, 182, 250, 2325, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
       "      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>([800, 122, 248, 282, 623, 272, 1256, 2210, 91...</td>\n",
       "      <td>([266, 23, 513, 143, 306, 78, 182, 250, 2325, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              SMILES  \\\n",
       "0  CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n",
       "1  CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n",
       "2  CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n",
       "3  CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n",
       "4  CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n",
       "\n",
       "                                     Target Sequence  Label  \\\n",
       "0  PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...    5.0   \n",
       "1  PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...    5.0   \n",
       "2  PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...    5.0   \n",
       "3  PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...    5.0   \n",
       "4  PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...    5.0   \n",
       "\n",
       "                                       drug_encoding  \\\n",
       "0  ([800, 122, 248, 282, 623, 272, 1256, 2210, 91...   \n",
       "1  ([800, 122, 248, 282, 623, 272, 1256, 2210, 91...   \n",
       "2  ([800, 122, 248, 282, 623, 272, 1256, 2210, 91...   \n",
       "3  ([800, 122, 248, 282, 623, 272, 1256, 2210, 91...   \n",
       "4  ([800, 122, 248, 282, 623, 272, 1256, 2210, 91...   \n",
       "\n",
       "                                     target_encoding  \n",
       "0  ([266, 23, 513, 143, 306, 78, 182, 250, 2325, ...  \n",
       "1  ([266, 23, 513, 143, 306, 78, 182, 250, 2325, ...  \n",
       "2  ([266, 23, 513, 143, 306, 78, 182, 250, 2325, ...  \n",
       "3  ([266, 23, 513, 143, 306, 78, 182, 250, 2325, ...  \n",
       "4  ([266, 23, 513, 143, 306, 78, 182, 250, 2325, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, val, test = utils.data_process(X_drugs, X_targets, y, \n",
    "                                drug_encoding, target_encoding, \n",
    "                                split_method='random',frac=[0.7,0.1,0.2],\n",
    "                                random_seed = 1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  21039\n",
      "valid size:  3006\n",
      "test size:  6011\n"
     ]
    }
   ],
   "source": [
    "print(\"train size: \", train.shape[0])\n",
    "print(\"valid size: \", val.shape[0])\n",
    "print(\"test size: \", test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug SMILES: \n",
      " CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC=C4)N\n",
      "Drug SMILES length: \n",
      " 51\n",
      "----------------------------------------\n",
      "Drug Encoding: \n",
      " (array([ 800,  122,  248,  282,  623,  272, 1256, 2210,   91,   85,  109,\n",
      "        119,   80,    8,  282,  861,  209,   19,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0]))\n",
      "non-zeros:  18\n",
      "----------------------------------------\n",
      "Target Sequence: \n",
      " PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGKKESSRHGGPHCNVFVEHEALQRPVASDFEPQGLSEAARWNSKENLLAGPSENDPNLFVALYDFVASGDNTLSITKGEKLRVLGYNHNGEWCEAQTKNGQGWVPSNYITPVNSLEKHSWYHGPVSRNAAEYLLSSGINGSFLVRESESSPGQRSISLRYEGRVYHYRINTASDGKLYVSSESRFNTLAELVHHHSTVADGLITTLHYPAPKRNKPTVYGVSPNYDKWEMERTDITMKHKLGGGQYGEVYEGVWKKYSLTVAVKTLKEDTMEVEEFLKEAAVMKEIKHPNLVQLLGVCTREPPFYIITEFMTYGNLLDYLRECNRQEVNAVVLLYMATQISSAMEYLEKKNFIHRDLAARNCLVGENHLVKVADFGLSRLMTGDTYTAHAGAKFPIKWTAPESLAYNKFSIKSDVWAFGVLLWEIATYGMSPYPGIDLSQVYELLEKDYRMERPEGCPEKVYELMRACWQWNPSDRPSFAEIHQAFETMFQESSISDEVEKELGKQGVRGAVSTLLQAPELPTKTRTSRRAAEHRDTTDVPEMPHSKGQGESDPLDHEPAVSPLLPRKERGPPEGGLNEDERLLPKDKKTNLFSALIKKKKKTAPTPPKRSSSFREMDGQPERRGAGEEEGRDISNGALAFTPLDTADPAKSPKPSNGAGVPNGALRESGGSGFRSPHLWKKSSTLTSSRLATGEEEGGGSSSKRFLRSCSASCVPHGAKDTEWRSVTLPRDLQSTGRQFDSSTFGGHKSEKPALPRKRAGENRSDQVTRGTVTPPPRLVKKNEEAADEVFKDIMESSPGSSPPNLTPKPLRRQVTVAPASGLPHKEEAGKGSALGTPAAAEPVTPTSKAGSGAPGGTSKGPAEESRVRRHKHSSESPGRDKGKLSRLKPAPPPPPAASAGKAGGKPSQSPSQEAAGEAVLGAKTKATSLVDAVNSDAAKPSQPGEGLKKPVLPATPKPQSAKPSGTPISPAPVPSTLPSASSALAGDQPSSTAFIPLISTRVSLRKTRQPPERIASGAITKGVVLDSTEALCLAISRNSEQMASHSAVLEAGKNLYTFCVSYVDSIQQMRNKFAFREAINKLENNLRELQICPATAGSGPAATQDFSKLLSSVKEISDIVQR\n",
      "Target Sequence length: \n",
      " 1167\n",
      "----------------------------------------\n",
      "Target Encoding: \n",
      " (array([ 266,   23,  513,  143,  306,   78,  182,  250, 2325,   98,   63,\n",
      "         15,  294,  289, 1610,  471, 1485,  232,   82,   41,  532,   33,\n",
      "        197,   32,  339,  396,  113,   54,  170,   27,  136,  284,  763,\n",
      "        102,  282, 3962,   26,  372,  126,   60,  540,   41,  795, 1952,\n",
      "         55,  317,   27,  150,  317,  295,  235,  453,   79,  632,   40,\n",
      "       4089,  267,   24,  739,   23,  237,   72,  181,   81,  105,   23,\n",
      "        680,  213, 1198,   97,   31,  201,  158,  554, 2674,   64,  162,\n",
      "        332,  774,  349,   22,  865,   56,  528,   88,   33,   63, 1535,\n",
      "        572,   18,  790, 1729,   24,  257,   92,  684,   73, 1650,  448,\n",
      "        189,  153,  811,  233,  148, 2956,  193, 3981,   79, 2051,  198,\n",
      "         83,  110, 1823,  193,  108,   64,  268,  150,   15,  302,  137,\n",
      "       1720,   79, 3020,   40,  273, 1833,  368,  790,  241,   44, 1033,\n",
      "        193,   39,  503,   60,  221,  137,  432,   56,   60,   26,  155,\n",
      "       3296,  175,   55,  120,  245,  184, 1003,  115,  204,  885,  337,\n",
      "        226,  108,   17,  348,   77,   70,    4,  242,  104,   97,   39,\n",
      "        298,  499,   59,  212,   33, 1161,  800,   44,  208,  187,  607,\n",
      "         26,  162,  142,   58,  247,   91,   74,  488,    3,  367,   37,\n",
      "          1,  904,  182, 1645,   41,   53,  178,   86,  477,  387,   31,\n",
      "        114,  217,   99,   86,  165,  241,  480,  298,   23,  258,   59,\n",
      "        108,  139,  304,   63,   22,  418,  253,  125,  306,  112,  257,\n",
      "        137,   18,  406, 2298, 1432,   30,  244,  173,  408,  426,   76,\n",
      "        225,   76,  965,  187, 1314,  263,  301,  104,   33,   71,   75,\n",
      "       3597,  312, 1270,  660,   39,  238,  958,   83,   30,  231,  116,\n",
      "       1369,   61,  332, 1901,   85,  269,   65,  186,  158,   82,  945,\n",
      "        165,  925,  170, 1035,  179,   25,  260,   60, 1636,  406,   29,\n",
      "        121,   75,  374,   14,  112,   44, 1013,   99,   27,  620,   44,\n",
      "        116,   83, 1372,  110,   33,  252,   70, 1525, 1404,  457,   41,\n",
      "         34,  146,  119,   71,   81, 3056,  695,  221, 1335,   53,  179,\n",
      "       1118,   81, 2516,   81,   27, 1087, 3514,    3, 1403,   91, 2018,\n",
      "       3567,  519,   37,  321,  342,  273,  335,  110,   56,  107,  203,\n",
      "         50, 2450,  124,  866,   96, 2488,  405,  260,   36,  129,   52,\n",
      "        161,  123,   33,  147,   32,  330,   88,  133,   27,  260,  110,\n",
      "         41,  247,  107,  289,   69,   78,  702,  715,   37,  504, 1011,\n",
      "         26,   75,  113,  112, 1887,   33,   63,   33,  115,   55,  109,\n",
      "        731,   61, 1311,  762,   50,  412,  330,   34,   41,  868,  251,\n",
      "        109,   26, 1490,  809,   94,  611,   45, 4113,   94, 1881,  279,\n",
      "         16,  200,   61, 4019,   33,  189,   63,  119,   82,  489,   37,\n",
      "        133,   83, 1199,  381, 3782,  254, 1118,  129,   76,  104, 2531,\n",
      "       1969,   53,  181,   59,   31, 3197, 1604,   26, 1118,   13,  668,\n",
      "         29, 1366,  507,  853,  133,  129,   53, 1006,  109,   71, 1233,\n",
      "        680,   35,   76,  286,   27,  584,  163,   33, 1133,  685,   71,\n",
      "        135,  397, 1415, 2622,   65,  166,  295,   46, 1129,  390,  106,\n",
      "         96,   27,  142,  598,  162,   88,  350,   50,  158, 2576,   41,\n",
      "        650, 2596, 2064, 1517,  256,   98, 2348,  154,   80,   70,   46,\n",
      "        718,  247,   55,  469,  212,  246,   59, 2814,  535, 1865,   99,\n",
      "        386,   33,  493,   71,  209,  120,   18,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "non-zeros:  502\n"
     ]
    }
   ],
   "source": [
    "first_row = train.iloc[0]\n",
    "print(\"Drug SMILES: \\n\", first_row[\"SMILES\"])\n",
    "print(\"Drug SMILES length: \\n\", len(first_row[\"SMILES\"]))\n",
    "print(\"-\"*40)\n",
    "this_drug_encoding = first_row[\"drug_encoding\"]\n",
    "print(\"Drug Encoding: \\n\", this_drug_encoding)\n",
    "print(\"non-zeros: \", this_drug_encoding[1].sum())\n",
    "print(\"-\"*40)\n",
    "print(\"Target Sequence: \\n\", first_row[\"Target Sequence\"])\n",
    "print(\"Target Sequence length: \\n\", len(first_row[\"Target Sequence\"]))\n",
    "print(\"-\"*40)\n",
    "this_target_encoding = first_row[\"target_encoding\"]\n",
    "print(\"Target Encoding: \\n\", this_target_encoding)\n",
    "print(\"non-zeros: \", this_target_encoding[1].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer encoding method:\n",
    "\n",
    "- Type: Byte Pair Encoding (BPE)\n",
    "\n",
    "- Vocab size: \n",
    "\n",
    "    - drug: 2657 generated from Chembl (SMILES)\n",
    "    - protein: 4101 generated from Uniprot (Amino Acid, one letter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**Model configuration generation**) Now, we initialize a model with its configuration. You can modify almost any hyper-parameters (e.g., learning rate, epoch, batch size), model parameters (e.g. hidden dimensions, filter size) and etc in this function. The supported configurations are listed here in this [link](https://github.com/kexinhuang12345/DeepPurpose/blob/e169e2f550694145077bb2af95a4031abe400a77/DeepPurpose/utils.py#L486).\n",
    "\n",
    "For the sake of example, we specify the epoch size to be 3, and set the model parameters to be small so that you can run on both CPUs & GPUs quickly and can proceed to the next steps. For a reference parameters, checkout the notebooks in the DEMO folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.generate_config(drug_encoding = drug_encoding, \n",
    "                         target_encoding = target_encoding, \n",
    "                         cls_hidden_dims = [1024,1024,512], \n",
    "                         LR = 0.001, \n",
    "                         batch_size = 8,\n",
    "                         hidden_dim_drug = 128,\n",
    "                         input_dim_drug=128, \n",
    "                         input_dim_protein=128,\n",
    "                         train_epoch=5\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_dim_drug': 2586,\n",
       " 'input_dim_protein': 4114,\n",
       " 'hidden_dim_drug': 128,\n",
       " 'hidden_dim_protein': 64,\n",
       " 'cls_hidden_dims': [1024, 1024, 512],\n",
       " 'batch_size': 8,\n",
       " 'train_epoch': 5,\n",
       " 'test_every_X_epoch': 20,\n",
       " 'LR': 0.001,\n",
       " 'drug_encoding': 'Transformer',\n",
       " 'target_encoding': 'Transformer',\n",
       " 'result_folder': './result/',\n",
       " 'binary': False,\n",
       " 'num_workers': 0,\n",
       " 'transformer_emb_size_drug': 128,\n",
       " 'transformer_num_attention_heads_drug': 8,\n",
       " 'transformer_intermediate_size_drug': 512,\n",
       " 'transformer_n_layer_drug': 8,\n",
       " 'transformer_dropout_rate': 0.1,\n",
       " 'transformer_attention_probs_dropout': 0.1,\n",
       " 'transformer_hidden_dropout_rate': 0.1,\n",
       " 'transformer_emb_size_target': 64,\n",
       " 'transformer_num_attention_heads_target': 4,\n",
       " 'transformer_intermediate_size_target': 256,\n",
       " 'transformer_n_layer_target': 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**Model initialization**) Next, we initialize a model using the above configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DeepPurpose.DTI.DBTA at 0x26da1924c88>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.model_initialize(**config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug encoder: \n",
      "----------------------------------------\n",
      "transformer(\n",
      "  (emb): Embeddings(\n",
      "    (word_embeddings): Embedding(2586, 128)\n",
      "    (position_embeddings): Embedding(50, 128)\n",
      "    (LayerNorm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): Encoder_MultipleLayers(\n",
      "    (layer): ModuleList(\n",
      "      (0): Encoder(\n",
      "        (attention): Attention(\n",
      "          (self): SelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): SelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): Intermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        )\n",
      "        (output): Output(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Encoder(\n",
      "        (attention): Attention(\n",
      "          (self): SelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): SelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): Intermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        )\n",
      "        (output): Output(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Encoder(\n",
      "        (attention): Attention(\n",
      "          (self): SelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): SelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): Intermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        )\n",
      "        (output): Output(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Encoder(\n",
      "        (attention): Attention(\n",
      "          (self): SelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): SelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): Intermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        )\n",
      "        (output): Output(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Encoder(\n",
      "        (attention): Attention(\n",
      "          (self): SelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): SelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): Intermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        )\n",
      "        (output): Output(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Encoder(\n",
      "        (attention): Attention(\n",
      "          (self): SelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): SelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): Intermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        )\n",
      "        (output): Output(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): Encoder(\n",
      "        (attention): Attention(\n",
      "          (self): SelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): SelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): Intermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        )\n",
      "        (output): Output(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): Encoder(\n",
      "        (attention): Attention(\n",
      "          (self): SelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): SelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): Intermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        )\n",
      "        (output): Output(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "num of params:  1923840\n",
      "----------------------------------------\n",
      "Target encoder: \n",
      "----------------------------------------\n",
      "transformer(\n",
      "  (emb): Embeddings(\n",
      "    (word_embeddings): Embedding(4114, 64)\n",
      "    (position_embeddings): Embedding(545, 64)\n",
      "    (LayerNorm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): Encoder_MultipleLayers(\n",
      "    (layer): ModuleList(\n",
      "      (0): Encoder(\n",
      "        (attention): Attention(\n",
      "          (self): SelfAttention(\n",
      "            (query): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (key): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): SelfOutput(\n",
      "            (dense): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): Intermediate(\n",
      "          (dense): Linear(in_features=64, out_features=256, bias=True)\n",
      "        )\n",
      "        (output): Output(\n",
      "          (dense): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Encoder(\n",
      "        (attention): Attention(\n",
      "          (self): SelfAttention(\n",
      "            (query): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (key): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (value): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): SelfOutput(\n",
      "            (dense): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): Intermediate(\n",
      "          (dense): Linear(in_features=64, out_features=256, bias=True)\n",
      "        )\n",
      "        (output): Output(\n",
      "          (dense): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "num of params:  398272\n"
     ]
    }
   ],
   "source": [
    "print(\"Drug encoder: \")\n",
    "print(\"-\"*40)\n",
    "print(model.model_drug)\n",
    "print(\"num of params: \", count_parameters(model.model_drug))\n",
    "print(\"-\"*40)\n",
    "print(\"Target encoder: \")\n",
    "print(\"-\"*40)\n",
    "print(model.model_protein)\n",
    "print(\"num of params: \", count_parameters(model.model_protein))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to obtain Transformer embeddings?**\n",
    "```\n",
    "\t### parameter v (tuple of length 2) is from utils.drug2emb_encoder \n",
    "\tdef forward(self, v):\n",
    "\t\te = v[0].long().to(device)\n",
    "\t\te_mask = v[1].long().to(device)\n",
    "\t\tex_e_mask = e_mask.unsqueeze(1).unsqueeze(2)\n",
    "\t\tex_e_mask = (1.0 - ex_e_mask) * -10000.0\n",
    "\n",
    "\t\temb = self.emb(e)\n",
    "\t\tencoded_layers = self.encoder(emb.float(), ex_e_mask.float())\n",
    "\t\treturn encoded_layers[:,0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to make prediction based on embeddings?**\n",
    "\n",
    "```\n",
    "\tdef forward(self, v_D, v_P):\n",
    "\t\t# each encoding\n",
    "\t\tv_D = self.model_drug(v_D)\n",
    "\t\tv_P = self.model_protein(v_P)\n",
    "\t\t# concatenate and classify\n",
    "\t\tv_f = torch.cat((v_D, v_P), 1)\n",
    "\t\tfor i, l in enumerate(self.predictor):\n",
    "\t\t\tif i==(len(self.predictor)-1):\n",
    "\t\t\t\tv_f = l(v_f)\n",
    "\t\t\telse:\n",
    "\t\t\t\tv_f = F.relu(self.dropout(l(v_f)))\n",
    "\t\treturn v_f\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**Model Training**) Next, it is ready to train, using the ```model.train``` function! If you do not have test set, you can just use ```model.train(train, val)```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 1 GPU!\n",
      "--- Data Preparation ---\n",
      "--- Go for Training ---\n",
      "Training at Epoch 1 iteration 0 with loss 31.8204. Total time 0.0 hours\n",
      "Training at Epoch 1 iteration 100 with loss 2.89585. Total time 0.00305 hours\n",
      "Training at Epoch 1 iteration 200 with loss 2.13076. Total time 0.00611 hours\n",
      "Training at Epoch 1 iteration 300 with loss 0.27958. Total time 0.00944 hours\n",
      "Training at Epoch 1 iteration 400 with loss 0.81973. Total time 0.0125 hours\n",
      "Training at Epoch 1 iteration 500 with loss 3.12658. Total time 0.01555 hours\n",
      "Training at Epoch 1 iteration 600 with loss 0.16494. Total time 0.01861 hours\n",
      "Training at Epoch 1 iteration 700 with loss 1.06940. Total time 0.02138 hours\n",
      "Training at Epoch 1 iteration 800 with loss 2.44695. Total time 0.02444 hours\n",
      "Training at Epoch 1 iteration 900 with loss 0.27630. Total time 0.0275 hours\n",
      "Training at Epoch 1 iteration 1000 with loss 1.00578. Total time 0.03027 hours\n",
      "Training at Epoch 1 iteration 1100 with loss 0.52503. Total time 0.03333 hours\n",
      "Training at Epoch 1 iteration 1200 with loss 0.50416. Total time 0.03611 hours\n",
      "Training at Epoch 1 iteration 1300 with loss 0.75415. Total time 0.03916 hours\n",
      "Training at Epoch 1 iteration 1400 with loss 1.29690. Total time 0.04194 hours\n",
      "Training at Epoch 1 iteration 1500 with loss 1.39177. Total time 0.045 hours\n",
      "Training at Epoch 1 iteration 1600 with loss 1.66527. Total time 0.04805 hours\n",
      "Training at Epoch 1 iteration 1700 with loss 2.62369. Total time 0.05083 hours\n",
      "Training at Epoch 1 iteration 1800 with loss 2.40775. Total time 0.05388 hours\n",
      "Training at Epoch 1 iteration 1900 with loss 2.96429. Total time 0.05694 hours\n",
      "Training at Epoch 1 iteration 2000 with loss 0.98030. Total time 0.06 hours\n",
      "Training at Epoch 1 iteration 2100 with loss 2.68716. Total time 0.06305 hours\n",
      "Training at Epoch 1 iteration 2200 with loss 1.18109. Total time 0.06583 hours\n",
      "Training at Epoch 1 iteration 2300 with loss 0.48207. Total time 0.06888 hours\n",
      "Training at Epoch 1 iteration 2400 with loss 1.98122. Total time 0.07194 hours\n",
      "Training at Epoch 1 iteration 2500 with loss 1.39371. Total time 0.07472 hours\n",
      "Training at Epoch 1 iteration 2600 with loss 1.26568. Total time 0.0775 hours\n",
      "Validation at Epoch 1 , MSE: 0.76878 , Pearson Correlation: 0.31191 with p-value: 7.97231 , Concordance Index: 0.63545\n",
      "Training at Epoch 2 iteration 0 with loss 0.60133. Total time 0.08111 hours\n",
      "Training at Epoch 2 iteration 100 with loss 0.36598. Total time 0.08388 hours\n",
      "Training at Epoch 2 iteration 200 with loss 0.94370. Total time 0.08694 hours\n",
      "Training at Epoch 2 iteration 300 with loss 1.28225. Total time 0.08972 hours\n",
      "Training at Epoch 2 iteration 400 with loss 0.81971. Total time 0.09277 hours\n",
      "Training at Epoch 2 iteration 500 with loss 1.55831. Total time 0.09555 hours\n",
      "Training at Epoch 2 iteration 600 with loss 0.57287. Total time 0.09861 hours\n",
      "Training at Epoch 2 iteration 700 with loss 0.47150. Total time 0.10166 hours\n",
      "Training at Epoch 2 iteration 800 with loss 0.38669. Total time 0.10472 hours\n",
      "Training at Epoch 2 iteration 900 with loss 2.69334. Total time 0.10777 hours\n",
      "Training at Epoch 2 iteration 1000 with loss 1.83364. Total time 0.11055 hours\n",
      "Training at Epoch 2 iteration 1100 with loss 0.28239. Total time 0.11361 hours\n",
      "Training at Epoch 2 iteration 1200 with loss 1.95825. Total time 0.11638 hours\n",
      "Training at Epoch 2 iteration 1300 with loss 1.02887. Total time 0.11944 hours\n",
      "Training at Epoch 2 iteration 1400 with loss 0.75123. Total time 0.12222 hours\n",
      "Training at Epoch 2 iteration 1500 with loss 0.63410. Total time 0.12527 hours\n",
      "Training at Epoch 2 iteration 1600 with loss 1.43668. Total time 0.12805 hours\n",
      "Training at Epoch 2 iteration 1700 with loss 0.81627. Total time 0.13111 hours\n",
      "Training at Epoch 2 iteration 1800 with loss 1.01183. Total time 0.13416 hours\n",
      "Training at Epoch 2 iteration 1900 with loss 1.39668. Total time 0.13722 hours\n",
      "Training at Epoch 2 iteration 2000 with loss 0.88765. Total time 0.14 hours\n",
      "Training at Epoch 2 iteration 2100 with loss 0.30558. Total time 0.14305 hours\n",
      "Training at Epoch 2 iteration 2200 with loss 0.53974. Total time 0.14583 hours\n",
      "Training at Epoch 2 iteration 2300 with loss 1.61355. Total time 0.14888 hours\n",
      "Training at Epoch 2 iteration 2400 with loss 0.89625. Total time 0.15194 hours\n",
      "Training at Epoch 2 iteration 2500 with loss 0.59174. Total time 0.155 hours\n",
      "Training at Epoch 2 iteration 2600 with loss 2.15123. Total time 0.15777 hours\n",
      "Validation at Epoch 2 , MSE: 0.91494 , Pearson Correlation: 0.29612 with p-value: 6.67204 , Concordance Index: 0.64942\n",
      "Training at Epoch 3 iteration 0 with loss 0.94801. Total time 0.16111 hours\n",
      "Training at Epoch 3 iteration 100 with loss 0.72691. Total time 0.16416 hours\n",
      "Training at Epoch 3 iteration 200 with loss 0.76711. Total time 0.16722 hours\n",
      "Training at Epoch 3 iteration 300 with loss 0.85106. Total time 0.17 hours\n",
      "Training at Epoch 3 iteration 400 with loss 0.29624. Total time 0.17305 hours\n",
      "Training at Epoch 3 iteration 500 with loss 0.09639. Total time 0.17583 hours\n",
      "Training at Epoch 3 iteration 600 with loss 1.48639. Total time 0.17888 hours\n",
      "Training at Epoch 3 iteration 700 with loss 0.46214. Total time 0.18194 hours\n",
      "Training at Epoch 3 iteration 800 with loss 0.64756. Total time 0.185 hours\n",
      "Training at Epoch 3 iteration 900 with loss 0.27455. Total time 0.18805 hours\n",
      "Training at Epoch 3 iteration 1000 with loss 0.47756. Total time 0.19083 hours\n",
      "Training at Epoch 3 iteration 1100 with loss 0.11945. Total time 0.19388 hours\n",
      "Training at Epoch 3 iteration 1200 with loss 0.73656. Total time 0.19694 hours\n",
      "Training at Epoch 3 iteration 1300 with loss 0.99909. Total time 0.19972 hours\n",
      "Training at Epoch 3 iteration 1400 with loss 0.90002. Total time 0.20277 hours\n",
      "Training at Epoch 3 iteration 1500 with loss 1.09066. Total time 0.20583 hours\n",
      "Training at Epoch 3 iteration 1600 with loss 0.70030. Total time 0.20888 hours\n",
      "Training at Epoch 3 iteration 1700 with loss 0.60886. Total time 0.21166 hours\n",
      "Training at Epoch 3 iteration 1800 with loss 1.52690. Total time 0.21472 hours\n",
      "Training at Epoch 3 iteration 1900 with loss 0.24119. Total time 0.21777 hours\n",
      "Training at Epoch 3 iteration 2000 with loss 0.30349. Total time 0.22083 hours\n",
      "Training at Epoch 3 iteration 2100 with loss 2.74886. Total time 0.22361 hours\n",
      "Training at Epoch 3 iteration 2200 with loss 0.98650. Total time 0.22666 hours\n",
      "Training at Epoch 3 iteration 2300 with loss 0.12509. Total time 0.23 hours\n",
      "Training at Epoch 3 iteration 2400 with loss 0.31094. Total time 0.23305 hours\n",
      "Training at Epoch 3 iteration 2500 with loss 0.69987. Total time 0.23611 hours\n",
      "Training at Epoch 3 iteration 2600 with loss 0.54702. Total time 0.23888 hours\n",
      "Validation at Epoch 3 , MSE: 0.74698 , Pearson Correlation: 0.32657 with p-value: 1.20030 , Concordance Index: 0.65883\n",
      "Training at Epoch 4 iteration 0 with loss 0.26067. Total time 0.2425 hours\n",
      "Training at Epoch 4 iteration 100 with loss 0.05090. Total time 0.24527 hours\n",
      "Training at Epoch 4 iteration 200 with loss 2.10522. Total time 0.24861 hours\n",
      "Training at Epoch 4 iteration 300 with loss 0.47504. Total time 0.25166 hours\n",
      "Training at Epoch 4 iteration 400 with loss 0.22243. Total time 0.25472 hours\n",
      "Training at Epoch 4 iteration 500 with loss 1.33933. Total time 0.2575 hours\n",
      "Training at Epoch 4 iteration 600 with loss 0.58045. Total time 0.26055 hours\n",
      "Training at Epoch 4 iteration 700 with loss 0.09319. Total time 0.26361 hours\n",
      "Training at Epoch 4 iteration 800 with loss 0.17786. Total time 0.26638 hours\n",
      "Training at Epoch 4 iteration 900 with loss 0.79186. Total time 0.26944 hours\n",
      "Training at Epoch 4 iteration 1000 with loss 0.68075. Total time 0.2725 hours\n",
      "Training at Epoch 4 iteration 1100 with loss 0.08806. Total time 0.27527 hours\n",
      "Training at Epoch 4 iteration 1200 with loss 0.21859. Total time 0.27833 hours\n",
      "Training at Epoch 4 iteration 1300 with loss 0.30380. Total time 0.28111 hours\n",
      "Training at Epoch 4 iteration 1400 with loss 0.98695. Total time 0.28416 hours\n",
      "Training at Epoch 4 iteration 1500 with loss 0.47162. Total time 0.28694 hours\n",
      "Training at Epoch 4 iteration 1600 with loss 0.61805. Total time 0.29 hours\n",
      "Training at Epoch 4 iteration 1700 with loss 0.32217. Total time 0.29277 hours\n",
      "Training at Epoch 4 iteration 1800 with loss 0.54280. Total time 0.29611 hours\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 4 iteration 1900 with loss 2.18415. Total time 0.29888 hours\n",
      "Training at Epoch 4 iteration 2000 with loss 1.76950. Total time 0.30194 hours\n",
      "Training at Epoch 4 iteration 2100 with loss 1.22098. Total time 0.305 hours\n",
      "Training at Epoch 4 iteration 2200 with loss 0.12180. Total time 0.30777 hours\n",
      "Training at Epoch 4 iteration 2300 with loss 1.54285. Total time 0.31083 hours\n",
      "Training at Epoch 4 iteration 2400 with loss 0.94193. Total time 0.31388 hours\n",
      "Training at Epoch 4 iteration 2500 with loss 0.41330. Total time 0.31694 hours\n",
      "Training at Epoch 4 iteration 2600 with loss 1.68336. Total time 0.31972 hours\n",
      "Validation at Epoch 4 , MSE: 0.82195 , Pearson Correlation: 0.32651 with p-value: 1.29138 , Concordance Index: 0.62396\n",
      "Training at Epoch 5 iteration 0 with loss 0.70040. Total time 0.32305 hours\n",
      "Training at Epoch 5 iteration 100 with loss 2.46765. Total time 0.32611 hours\n",
      "Training at Epoch 5 iteration 200 with loss 0.23667. Total time 0.32916 hours\n",
      "Training at Epoch 5 iteration 300 with loss 1.26939. Total time 0.33222 hours\n",
      "Training at Epoch 5 iteration 400 with loss 3.03399. Total time 0.33555 hours\n",
      "Training at Epoch 5 iteration 500 with loss 0.24429. Total time 0.33833 hours\n",
      "Training at Epoch 5 iteration 600 with loss 1.06011. Total time 0.34138 hours\n",
      "Training at Epoch 5 iteration 700 with loss 0.17510. Total time 0.34416 hours\n",
      "Training at Epoch 5 iteration 800 with loss 1.38302. Total time 0.34722 hours\n",
      "Training at Epoch 5 iteration 900 with loss 0.28327. Total time 0.35 hours\n",
      "Training at Epoch 5 iteration 1000 with loss 0.73993. Total time 0.35305 hours\n",
      "Training at Epoch 5 iteration 1100 with loss 0.19583. Total time 0.35583 hours\n",
      "Training at Epoch 5 iteration 1200 with loss 0.33498. Total time 0.35888 hours\n",
      "Training at Epoch 5 iteration 1300 with loss 0.59034. Total time 0.36194 hours\n",
      "Training at Epoch 5 iteration 1400 with loss 0.20500. Total time 0.36472 hours\n",
      "Training at Epoch 5 iteration 1500 with loss 0.18793. Total time 0.36777 hours\n",
      "Training at Epoch 5 iteration 1600 with loss 1.60543. Total time 0.37083 hours\n",
      "Training at Epoch 5 iteration 1700 with loss 0.43030. Total time 0.37388 hours\n",
      "Training at Epoch 5 iteration 1800 with loss 0.82511. Total time 0.37666 hours\n",
      "Training at Epoch 5 iteration 1900 with loss 0.32931. Total time 0.38 hours\n",
      "Training at Epoch 5 iteration 2000 with loss 0.37594. Total time 0.38277 hours\n",
      "Training at Epoch 5 iteration 2100 with loss 2.24358. Total time 0.38583 hours\n",
      "Training at Epoch 5 iteration 2200 with loss 0.48952. Total time 0.38888 hours\n",
      "Training at Epoch 5 iteration 2300 with loss 0.41386. Total time 0.39194 hours\n",
      "Training at Epoch 5 iteration 2400 with loss 0.58426. Total time 0.395 hours\n",
      "Training at Epoch 5 iteration 2500 with loss 0.29747. Total time 0.39777 hours\n",
      "Training at Epoch 5 iteration 2600 with loss 0.39164. Total time 0.40111 hours\n",
      "Validation at Epoch 5 , MSE: 0.74581 , Pearson Correlation: 0.32895 with p-value: 8.64491 , Concordance Index: 0.65828\n",
      "--- Go for Testing ---\n",
      "Testing MSE: 0.7523888673350859 , Pearson Correlation: 0.31224382554610175 with p-value: 4.6130639470689444e-136 , Concordance Index: 0.6430804218589411\n",
      "--- Training Finished ---\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train(train, val, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model will automatically generate and plot the training process, along with the validation result and test result.\n",
    "\n",
    "(**Model Prediction and Repuposing/Screening**) Next, we see how we can predict affinity scores on new data. Suppose the new data is a drug-target pair below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug Target Interaction Prediction Mode...\n",
      "in total: 1 drug-target pairs\n",
      "encoding drug...\n",
      "unique drugs: 1\n",
      "encoding protein...\n",
      "unique target sequence: 1\n",
      "splitting dataset...\n",
      "do not do train/test split on the data for already splitted data\n",
      "predicting...\n",
      "The predicted score is [5.725095748901367]\n"
     ]
    }
   ],
   "source": [
    "X_drug = ['CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC=C4)N']\n",
    "X_target = ['MKKFFDSRREQGGSGLGSGSSGGGGSTSGLGSGYIGRVFGIGRQQVTVDEVLAEGGFAIVFLVRTSNGMKCALKRMFVNNEHDLQVCKREIQIMRDLSGHKNIVGYIDSSINNVSSGDVWEVLILMDFCRGGQVVNLMNQRLQTGFTENEVLQIFCDTCEAVARLHQCKTPIIHRDLKVENILLHDRGHYVLCDFGSATNKFQNPQTEGVNAVEDEIKKYTTLSYRAPEMVNLYSGKIITTKADIWALGCLLYKLCYFTLPFGESQVAICDGNFTIPDNSRYSQDMHCLIRYMLEPDPDKRPDIYQVSYFSFKLLKKECPIPNVQNSPIPAKLPEPVKASEAAAKKTQPKARLTDPIPTTETSIAPRQRPKAGQTQPNPGILPIQPALTPRKRATVQPPPQAAGSSNQPGLLASVPQPKPQAPPSQPLPQTQAKQPQAPPTPQQTPSTQAQGLPAQAQATPQHQQQLFLKQQQQQQQPPPAQQQPAGTFYQQQQAQTQQFQAVHPATQKPAIAQFPVVSQGGSQQQLMQNFYQQQQQQQQQQQQQQLATALHQQQLMTQQAALQQKPTMAAGQQPQPQPAAAPQPAPAQEPAIQAPVRQQPKVQTTPPPAVQGQKVGSLTPPSSPKTQRAGHRRILSDVTHSAVFGVPASKSTQLLQAAAAEASLNKSKSATTTPSGSPRTSQQNVYNPSEGSTWNPFDDDNFSKLTAEELLNKDFAKLGEGKHPEKLGGSAESLIPGFQSTQGDAFATTSFSAGTAEKRKGGQTVDSGLPLLSVSDPFIPLQVPDAPEKLIEGLKSPDTSLLLPDLLPMTDPFGSTSDAVIEKADVAVESLIPGLEPPVPQRLPSQTESVTSNRTDSLTGEDSLLDCSLLSNPTTDLLEEFAPTAISAPVHKAAEDSNLISGFDVPEGSDKVAEDEFDPIPVLITKNPQGGHSRNSSGSSESSLPNLARSLLLVDQLIDL']\n",
    "y = [7.365]\n",
    "X_pred = utils.data_process(X_drug, X_target, y, \n",
    "                                drug_encoding, target_encoding, \n",
    "                                split_method='no_split')\n",
    "y_pred = model.predict(X_pred)\n",
    "print('The predicted score is ' + str(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do repurposing and screening using the trained model. Basically, for repurposing a set of existing drugs (**r**) for a single new target (*t*), we run the above prediction function after pairing each repurposing drug with the target. Similarly, for screening, we instead have a set of drug-target pairs (**d**, **t**). We wrap the operation into a ```models.repurpose``` and ```models.virtual_screening``` methods.\n",
    "\n",
    "For example, suppose we want to do repurposing from a set of antiviral drugs for a COVID-19 target 3CL protease. The corresponding data can be retrieved using ```dataset``` functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Name: SARS-CoV2 3CL Protease\n",
      "Amino Acid Sequence: SGFRKMAFPSGKVEGCMVQVTCGTTTLNGLWLDDVVYCPRHVICTSEDMLNPNYEDLLIRKSNHNFLVQAGNVQLRVIGHSMQNCVLKLKVDTANPKTPKYKFVRIQPGQTFSVLACYNGSPSGVYQCAMRPNFTIKGSFLNGSCGSVGFNIDYDCVSFCYMHHMELPTGVHAGTDLEGNFYGPFVDRQTAQAAGTDTTITVNVLAWLYAAVINGDRWFLNRFTTTLNDFNLVAMKYNYEPLTQDHVDILGPLSAQTGIAVLDMCASLKELLQNGMNGRTILGSALLEDEFTPFDVVRQCSGVTFQ\n"
     ]
    }
   ],
   "source": [
    "t, t_name = dataset.load_SARS_CoV2_Protease_3CL()\n",
    "print('Target Name: ' + t_name)\n",
    "print('Amino Acid Sequence: '+ t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of drugs to for repurposing:  82\n",
      "Repurposing Drug 1 Name: Abacavir\n",
      "Repurposing Drug 1 SMILES: C1CC1NC2=C3C(=NC(=N2)N)N(C=N3)C4CC(C=C4)CO\n",
      "Repurposing Drug 1 Pubchem CID: 441300\n"
     ]
    }
   ],
   "source": [
    "r, r_name, r_pubchem_cid = dataset.load_antiviral_drugs()\n",
    "print(\"Total number of drugs to for repurposing: \", len(r))\n",
    "print('Repurposing Drug 1 Name: ' + r_name[0])\n",
    "print('Repurposing Drug 1 SMILES: ' + r[0])\n",
    "print('Repurposing Drug 1 Pubchem CID: ' + str(r_pubchem_cid[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can call the ```repurpose``` function. After feeding the necessary inputs, it will print a list of repurposed drugs ranked on its affinity to the target protein. The ```convert_y``` parameter should be set to be ```False``` when the ranking is ascending (i.e. lower value -> higher affinity) due to the log transformation, vice versus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CCC(C)C(C(=O)NC(CCC(=O)O)C(=O)NC(CCC(=O)O)C(=O)NC(CO)C(=O)NC(CCC(=O)N)C(=O)NC(CC(=O)N)C(=O)NC(CCC(=O)N)C(=O)NC(CCC(=O)N)C(=O)NC(CCC(=O)O)C(=O)NC(CCCCN)C(=O)NC(CC(=O)N)C(=O)NC(CCC(=O)O)C(=O)NC(CCC(=O)N)C(=O)NC(CCC(=O)O)C(=O)NC(CC(C)C)C(=O)NC(CC(C)C)C(=O)NC(CCC(=O)O)C(=O)NC(CC(C)C)C(=O)NC(CC(=O)O)C(=O)NC(CCCCN)C(=O)NC(CC1=CNC2=CC=CC=C21)C(=O)NC(C)C(=O)NC(CO)C(=O)NC(CC(C)C)C(=O)NC(CC3=CNC4=CC=CC=C43)C(=O)NC(CC(=O)N)C(=O)NC(CC5=CNC6=CC=CC=C65)C(=O)NC(CC7=CC=CC=C7)C(=O)N)NC(=O)C(CC(C)C)NC(=O)C(CO)NC(=O)C(CC8=CN=CN8)NC(=O)C(C(C)CC)NC(=O)C(CC(C)C)NC(=O)C(CO)NC(=O)C(C(C)O)NC(=O)C(CC9=CC=C(C=C9)O)NC(=O)C'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "r[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repurposing...\n",
      "Drug Target Interaction Prediction Mode...\n",
      "in total: 81 drug-target pairs\n",
      "encoding drug...\n",
      "unique drugs: 80\n",
      "encoding protein...\n",
      "unique target sequence: 1\n",
      "Done.\n",
      "predicting...\n",
      "---------------\n",
      "Drug Repurposing Result for SARS-CoV2 3CL Protease\n",
      "+------+----------------------+------------------------+---------------+\n",
      "| Rank |      Drug Name       |      Target Name       | Binding Score |\n",
      "+------+----------------------+------------------------+---------------+\n",
      "|  1   |       Abacavir       | SARS-CoV2 3CL Protease |    3774.70    |\n",
      "|  2   |      Aciclovir       | SARS-CoV2 3CL Protease |    3774.70    |\n",
      "|  3   |       Adefovir       | SARS-CoV2 3CL Protease |    3774.70    |\n",
      "|  4   |      Amantadine      | SARS-CoV2 3CL Protease |    3774.70    |\n",
      "|  5   |      Amprenavir      | SARS-CoV2 3CL Protease |    3774.70    |\n",
      "|  6   |       Arbidol        | SARS-CoV2 3CL Protease |    3774.70    |\n",
      "|  7   |      Atazanavir      | SARS-CoV2 3CL Protease |    3774.70    |\n",
      "|  8   |      Baloxavir       | SARS-CoV2 3CL Protease |    3774.70    |\n",
      "|  9   |     Bictegravir      | SARS-CoV2 3CL Protease |    3774.70    |\n",
      "|  10  |    Emtricitabine     | SARS-CoV2 3CL Protease |    3774.70    |\n",
      "checkout ./result/repurposing.txt for the whole list\n"
     ]
    }
   ],
   "source": [
    "y_pred = models.repurpose(X_repurpose = np.concatenate([r[0:27], r[28:]]), target = t, model = model, drug_names = r_name, target_name = t_name, \n",
    "                          result_folder = \"./result/\", convert_y = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's move on to showcase how to do virtual screening. We first load a sample of data from BindingDB dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n"
     ]
    }
   ],
   "source": [
    "t, d = dataset.load_IC50_1000_Samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target length:  100\n",
      "drug length:  100\n"
     ]
    }
   ],
   "source": [
    "print(\"target length: \", len(t))\n",
    "print(\"drug length: \", len(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the ```virtual_screening``` function to generate a list of drug-target pairs that have high binding affinities. If no drug/target names are provided, the index of the drug/target list is used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virtual screening...\n",
      "Drug Target Interaction Prediction Mode...\n",
      "in total: 100 drug-target pairs\n",
      "encoding drug...\n",
      "unique drugs: 100\n",
      "encoding protein...\n",
      "unique target sequence: 91\n",
      "Done.\n",
      "predicting...\n",
      "---------------\n",
      "Virtual Screening Result\n",
      "+------+-----------+-------------+---------------+\n",
      "| Rank | Drug Name | Target Name | Binding Score |\n",
      "+------+-----------+-------------+---------------+\n",
      "|  1   |  Drug 60  |  Target 60  |      6.28     |\n",
      "|  2   |  Drug 21  |  Target 21  |      6.13     |\n",
      "|  3   |  Drug 92  |  Target 92  |      5.82     |\n",
      "|  4   |  Drug 20  |  Target 20  |      5.79     |\n",
      "|  5   |  Drug 12  |  Target 12  |      5.75     |\n",
      "|  6   |   Drug 0  |   Target 0  |      5.67     |\n",
      "|  7   |   Drug 2  |   Target 2  |      5.65     |\n",
      "|  8   |  Drug 87  |  Target 87  |      5.64     |\n",
      "|  9   |   Drug 7  |   Target 7  |      5.61     |\n",
      "|  10  |   Drug 8  |   Target 8  |      5.58     |\n",
      "checkout ./result/virtual_screening.txt for the whole list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = models.virtual_screening(d, t, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and loading models are also really easy. The loading function also automatically detects if the model is trained on multiple GPUs. To save a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model('./model_tm_tm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a saved/pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DeepPurpose.DTI.DBTA at 0x26da4cf8b48>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.model_pretrained(path_dir = './model_tm_tm')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also provided a list of pretrained model, you can find all available ones under the [list](https://github.com/kexinhuang12345/DeepPurpose#pretrained-models). For example, to load a MPNN+CNN model pretrained on BindingDB Kd dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded in the local system...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DeepPurpose.DTI.DBTA at 0x29182ed3588>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.model_pretrained(model = 'MPNN_CNN_BindingDB')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPNN(\n",
      "  (W_i): Linear(in_features=50, out_features=128, bias=False)\n",
      "  (W_h): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (W_o): Linear(in_features=167, out_features=128, bias=True)\n",
      ")\n",
      "num of params:  44288\n",
      "----------------------------------------\n",
      "CNN(\n",
      "  (conv): ModuleList(\n",
      "    (0): Conv1d(26, 32, kernel_size=(4,), stride=(1,))\n",
      "    (1): Conv1d(32, 64, kernel_size=(8,), stride=(1,))\n",
      "    (2): Conv1d(64, 96, kernel_size=(12,), stride=(1,))\n",
      "  )\n",
      "  (fc1): Linear(in_features=96, out_features=256, bias=True)\n",
      ")\n",
      "num of params:  118464\n"
     ]
    }
   ],
   "source": [
    "print(model.model_drug)\n",
    "print(\"num of params: \", count_parameters(model.model_drug))\n",
    "print(\"-\"*40)\n",
    "print(model.model_protein)\n",
    "print(\"num of params: \", count_parameters(model.model_protein))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repurposing...\n",
      "Drug Target Interaction Prediction Mode...\n",
      "in total: 81 drug-target pairs\n",
      "encoding drug...\n",
      "unique drugs: 80\n",
      "encoding protein...\n",
      "unique target sequence: 74\n",
      "Done.\n",
      "predicting...\n",
      "---------------\n",
      "Drug Repurposing Result for SARS-CoV2 3CL Protease\n",
      "+------+----------------------+------------------------+---------------+\n",
      "| Rank |      Drug Name       |      Target Name       | Binding Score |\n",
      "+------+----------------------+------------------------+---------------+\n",
      "|  1   |      Entecavir       | SARS-CoV2 3CL Protease |      0.00     |\n",
      "|  2   |     Methisazone      | SARS-CoV2 3CL Protease |      0.30     |\n",
      "|  3   |      Aciclovir       | SARS-CoV2 3CL Protease |      0.77     |\n",
      "|  4   |     Grazoprevir      | SARS-CoV2 3CL Protease |      4.50     |\n",
      "|  5   |      Boceprevir      | SARS-CoV2 3CL Protease |      5.94     |\n",
      "|  6   |       Loviride       | SARS-CoV2 3CL Protease |      7.85     |\n",
      "|  7   |      Lamivudine      | SARS-CoV2 3CL Protease |     10.16     |\n",
      "|  8   |     Telbivudine      | SARS-CoV2 3CL Protease |     13.89     |\n",
      "|  9   |      Darunavir       | SARS-CoV2 3CL Protease |     14.89     |\n",
      "|  10  |      Peramivir       | SARS-CoV2 3CL Protease |     42.07     |\n",
      "checkout ./result/repurposing.txt for the whole list\n"
     ]
    }
   ],
   "source": [
    "y_pred = models.repurpose(X_repurpose = np.concatenate([r[0:27], r[28:]]), target = t, model = model, drug_names = r_name, target_name = t_name, \n",
    "                          result_folder = \"./result/\", convert_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virtual screening...\n",
      "Drug Target Interaction Prediction Mode...\n",
      "in total: 100 drug-target pairs\n",
      "encoding drug...\n",
      "unique drugs: 100\n",
      "encoding protein...\n",
      "unique target sequence: 91\n",
      "Done.\n",
      "predicting...\n",
      "---------------\n",
      "Virtual Screening Result\n",
      "+------+-----------+-------------+---------------+\n",
      "| Rank | Drug Name | Target Name | Binding Score |\n",
      "+------+-----------+-------------+---------------+\n",
      "|  1   |  Drug 40  |  Target 40  |     20.17     |\n",
      "|  2   |  Drug 81  |  Target 81  |     12.67     |\n",
      "|  3   |  Drug 87  |  Target 87  |     10.02     |\n",
      "|  4   |  Drug 88  |  Target 88  |      9.62     |\n",
      "|  5   |  Drug 37  |  Target 37  |      8.92     |\n",
      "|  6   |  Drug 82  |  Target 82  |      8.81     |\n",
      "|  7   |  Drug 90  |  Target 90  |      8.60     |\n",
      "|  8   |  Drug 71  |  Target 71  |      8.42     |\n",
      "|  9   |  Drug 14  |  Target 14  |      8.39     |\n",
      "|  10  |  Drug 39  |  Target 39  |      8.24     |\n",
      "checkout ./result/virtual_screening.txt for the whole list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = models.virtual_screening(d, t, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded in the local system...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DeepPurpose.DTI.DBTA at 0x26da1690888>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.model_pretrained(model = 'Transformer_CNN_BindingDB')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer(\n",
      "  (emb): Embeddings(\n",
      "    (word_embeddings): Embedding(2586, 128)\n",
      "    (position_embeddings): Embedding(50, 128)\n",
      "    (LayerNorm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): Encoder_MultipleLayers(\n",
      "    (layer): ModuleList(\n",
      "      (0): Encoder(\n",
      "        (attention): Attention(\n",
      "          (self): SelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): SelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): Intermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        )\n",
      "        (output): Output(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Encoder(\n",
      "        (attention): Attention(\n",
      "          (self): SelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): SelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): Intermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        )\n",
      "        (output): Output(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Encoder(\n",
      "        (attention): Attention(\n",
      "          (self): SelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): SelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): Intermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        )\n",
      "        (output): Output(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Encoder(\n",
      "        (attention): Attention(\n",
      "          (self): SelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): SelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): Intermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        )\n",
      "        (output): Output(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Encoder(\n",
      "        (attention): Attention(\n",
      "          (self): SelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): SelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): Intermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        )\n",
      "        (output): Output(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Encoder(\n",
      "        (attention): Attention(\n",
      "          (self): SelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): SelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): Intermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        )\n",
      "        (output): Output(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): Encoder(\n",
      "        (attention): Attention(\n",
      "          (self): SelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): SelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): Intermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        )\n",
      "        (output): Output(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): Encoder(\n",
      "        (attention): Attention(\n",
      "          (self): SelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): SelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): Intermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "        )\n",
      "        (output): Output(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "num of params:  1923840\n",
      "----------------------------------------\n",
      "CNN(\n",
      "  (conv): ModuleList(\n",
      "    (0): Conv1d(26, 32, kernel_size=(4,), stride=(1,))\n",
      "    (1): Conv1d(32, 64, kernel_size=(8,), stride=(1,))\n",
      "    (2): Conv1d(64, 96, kernel_size=(12,), stride=(1,))\n",
      "  )\n",
      "  (fc1): Linear(in_features=96, out_features=256, bias=True)\n",
      ")\n",
      "num of params:  118464\n"
     ]
    }
   ],
   "source": [
    "print(model.model_drug)\n",
    "print(\"num of params: \", count_parameters(model.model_drug))\n",
    "print(\"-\"*40)\n",
    "print(model.model_protein)\n",
    "print(\"num of params: \", count_parameters(model.model_protein))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repurposing...\n",
      "Drug Target Interaction Prediction Mode...\n",
      "in total: 81 drug-target pairs\n",
      "encoding drug...\n",
      "unique drugs: 80\n",
      "encoding protein...\n",
      "unique target sequence: 74\n",
      "Done.\n",
      "predicting...\n",
      "---------------\n",
      "Drug Repurposing Result for SARS-CoV2 3CL Protease\n",
      "+------+----------------------+------------------------+---------------+\n",
      "| Rank |      Drug Name       |      Target Name       | Binding Score |\n",
      "+------+----------------------+------------------------+---------------+\n",
      "|  1   |      Zidovudine      | SARS-CoV2 3CL Protease |      0.69     |\n",
      "|  2   |      Moroxydine      | SARS-CoV2 3CL Protease |      1.73     |\n",
      "|  3   |      Efavirenz       | SARS-CoV2 3CL Protease |      6.97     |\n",
      "|  4   |      Tipranavir      | SARS-CoV2 3CL Protease |      9.22     |\n",
      "|  5   |       Arbidol        | SARS-CoV2 3CL Protease |     11.61     |\n",
      "|  6   |      Entecavir       | SARS-CoV2 3CL Protease |     17.38     |\n",
      "|  7   |      Baloxavir       | SARS-CoV2 3CL Protease |     20.57     |\n",
      "|  8   |      Didanosine      | SARS-CoV2 3CL Protease |     24.12     |\n",
      "|  9   |      Ritonavir       | SARS-CoV2 3CL Protease |     41.71     |\n",
      "|  10  |     Famciclovir      | SARS-CoV2 3CL Protease |     41.80     |\n",
      "checkout ./result/repurposing.txt for the whole list\n"
     ]
    }
   ],
   "source": [
    "y_pred = models.repurpose(X_repurpose = np.concatenate([r[0:27], r[28:]]), target = t, model = model, drug_names = r_name, target_name = t_name, \n",
    "                          result_folder = \"./result/\", convert_y = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provided many more functionalities for DTI research purposes. \n",
    "\n",
    "For example, this [demo](https://github.com/kexinhuang12345/DeepPurpose/blob/master/DEMO/Drug_Property_Pred-Ax-Hyperparam-Tune.ipynb) shows how to use Ax platform to do some latest hyperparameter tuning methods such as Bayesian Optimization on DeepPurpose.\n",
    "\n",
    "Model robustness is very important for DTI task. One way to measure is to see how the model can predict drug or protein that do not exist in the training set, i.e., cold drug/target setting. You can achieve this by modifying the ```split_method``` parameter in the ```data_process``` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virtual screening...\n",
      "Drug Target Interaction Prediction Mode...\n",
      "in total: 100 drug-target pairs\n",
      "encoding drug...\n",
      "unique drugs: 100\n",
      "encoding protein...\n",
      "unique target sequence: 91\n",
      "Done.\n",
      "predicting...\n",
      "---------------\n",
      "Virtual Screening Result\n",
      "+------+-----------+-------------+---------------+\n",
      "| Rank | Drug Name | Target Name | Binding Score |\n",
      "+------+-----------+-------------+---------------+\n",
      "|  1   |   Drug 7  |   Target 7  |      9.01     |\n",
      "|  2   |  Drug 37  |  Target 37  |      8.42     |\n",
      "|  3   |  Drug 28  |  Target 28  |      8.02     |\n",
      "|  4   |  Drug 72  |  Target 72  |      7.92     |\n",
      "|  5   |  Drug 81  |  Target 81  |      7.88     |\n",
      "|  6   |  Drug 97  |  Target 97  |      7.85     |\n",
      "|  7   |  Drug 43  |  Target 43  |      7.79     |\n",
      "|  8   |  Drug 79  |  Target 79  |      7.55     |\n",
      "|  9   |  Drug 92  |  Target 92  |      7.54     |\n",
      "|  10  |  Drug 15  |  Target 15  |      7.53     |\n",
      "checkout ./result/virtual_screening.txt for the whole list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = models.virtual_screening(d, t, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Processing...\n",
      "100% [............................................................................] 179878 / 179878Beginning to extract zip file...\n",
      "Default set to logspace (nM -> p) for easier regression\n",
      "Done!\n",
      "Drug Target Interaction Prediction Mode...\n",
      "in total: 30056 drug-target pairs\n",
      "encoding drug...\n",
      "unique drugs: 68\n",
      "encoding protein...\n",
      "unique target sequence: 379\n",
      "splitting dataset...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "X_drugs, X_targets, y = dataset.load_process_DAVIS(path = './data', binary = False, convert_to_log = True, threshold = 30)\n",
    "train, val, test = utils.data_process(X_drugs, X_targets, y, \n",
    "                                drug_encoding, target_encoding, \n",
    "                                split_method='cold_drug',frac=[0.7,0.1,0.2],\n",
    "                                random_seed = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That wraps up our tutorials on the main functionalities of DeepPurpose's Drug Target Interaction Prediction framework! \n",
    "\n",
    "Do checkout the upcoming tutorials:\n",
    "\n",
    "Tutorial 2: Drug Property Prediction using DeepPurpose\n",
    "\n",
    "Tutorial 3: Repurposing and Virtual Screening Using One Line of Code\n",
    "\n",
    "**Star & watch & contribute to DeepPurpose's [github repository](https://github.com/kexinhuang12345/DeepPurpose)!**\n",
    "\n",
    "Feedbacks would also be appreciated and you can send me an email (kexinhuang@hsph.harvard.edu)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
